{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a446a646",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis and Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, mean_squared_error, r2_score, confusion_matrix)\n",
    "\n",
    "# Load CSV\n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# Tiền xử lý cơ bản\n",
    "def basic_clean(df, drop_cols=None):\n",
    "    df = df.copy()\n",
    "    if drop_cols:\n",
    "        df.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[c].fillna(df[c].median(), inplace=True)\n",
    "    for c in df.select_dtypes(exclude=[np.number]).columns:\n",
    "        df[c].fillna(df[c].mode().iloc[0], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Xác định X, y theo loại bài toán\n",
    "def define_xy(df, task_type, target=None):\n",
    "    if task_type == 'regression':\n",
    "        X = df.drop(columns=[target])\n",
    "        y = df[target]\n",
    "        return X, y\n",
    "\n",
    "    if task_type == 'classification':\n",
    "        y_raw = df[target]\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y_raw)\n",
    "        X = df.drop(columns=[target])\n",
    "        return X, y\n",
    "\n",
    "    if task_type == 'unsupervised':\n",
    "        return df, None\n",
    "\n",
    "    raise ValueError(\"task_type phải là 'regression', 'classification' hoặc 'unsupervised'\")\n",
    "\n",
    "# Chia dữ liệu huấn luyện – validation – test\n",
    "def split_xy(X, y=None, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    if y is None:\n",
    "        return train_test_split(X, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    rel_val = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full,\n",
    "        test_size=rel_val, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Scaling\n",
    "def scale_data(X_train, X_val=None, X_test=None, method='standard'):\n",
    "    scaler = StandardScaler() if method=='standard' else MinMaxScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val) if X_val is not None else None\n",
    "    X_test_s = scaler.transform(X_test) if X_test is not None else None\n",
    "    return scaler, X_train_s, X_val_s, X_test_s\n",
    "\n",
    "# Lưu / tải model\n",
    "def save_model(model, path):\n",
    "    joblib.dump(model, path)\n",
    "\n",
    "def load_model(path):\n",
    "    return joblib.load(path)\n",
    "\n",
    "# Các hàm đánh giá chung\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    return {'mse': mean_squared_error(y_true,y_pred), 'rmse': np.sqrt(mean_squared_error(y_true,y_pred)), 'r2': r2_score(y_true,y_pred)}\n",
    "\n",
    "def classification_metrics(y_true, y_pred, y_prob=None):\n",
    "    out = {\n",
    "        'accuracy': accuracy_score(y_true,y_pred),\n",
    "        'precision': precision_score(y_true,y_pred, average='binary' if len(np.unique(y_true))==2 else 'macro', zero_division=0),\n",
    "        'recall': recall_score(y_true,y_pred, average='binary' if len(np.unique(y_true))==2 else 'macro', zero_division=0),\n",
    "        'f1': f1_score(y_true,y_pred, average='binary' if len(np.unique(y_true))==2 else 'macro', zero_division=0)\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            out['roc_auc'] = roc_auc_score(y_true, y_prob)\n",
    "        except Exception:\n",
    "            out['roc_auc'] = None\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6200aec6",
   "metadata": {},
   "source": [
    "# 2. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdbb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load & preprocess\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='regression', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "\n",
    "# Train\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_s, y_train)\n",
    "\n",
    "# Evaluate\n",
    "pred = model.predict(X_test_s)\n",
    "print(regression_metrics(y_test, pred))\n",
    "save_model({'model': model, 'scaler': scaler}, '../models/linear_regression.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed0184",
   "metadata": {},
   "source": [
    "# 3. Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load & preprocess\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='regression', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "# Polynomial Regression with Grid Search\n",
    "pipe = Pipeline([\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LinearRegression())\n",
    "])\n",
    "param_grid = {'poly__degree': [2,3], 'poly__include_bias':[False]}\n",
    "search = GridSearchCV(pipe, param_grid, cv=5, scoring='r2')\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_, search.best_score_)\n",
    "save_model(search.best_estimator_, '../models/poly_regression.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2a80c",
   "metadata": {},
   "source": [
    "# 4. Regularized Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load & preprocess\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='regression', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "# Train & evaluate regularized regression models\n",
    "models = {\n",
    "    'ridge': (Ridge(), {'alpha':[0.1,1,10]}),\n",
    "    'lasso': (Lasso(max_iter=5000), {'alpha':[0.001,0.01,0.1,1]}),\n",
    "    'elastic': (ElasticNet(max_iter=5000), {'alpha':[0.01,0.1,1], 'l1_ratio':[0.2,0.5,0.8]})\n",
    "}\n",
    "for name,(est,params) in models.items():\n",
    "    grid = GridSearchCV(est, params, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid.fit(X_train_s, y_train)\n",
    "    print(name, grid.best_params_, -grid.best_score_)\n",
    "    save_model(grid.best_estimator_, f'../models/{name}.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ce25b",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901dbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Prepare data \n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "param = {'C':[0.01,0.1,1,10], 'penalty':['l2'], 'solver':['lbfgs']}\n",
    "gs = GridSearchCV(clf, param, cv=5, scoring='roc_auc')\n",
    "gs.fit(X_train_s, y_train)\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "probs = gs.predict_proba(X_test_s)[:,1]\n",
    "preds = gs.predict(X_test_s)\n",
    "print(classification_metrics(y_test, preds, probs))\n",
    "save_model(gs.best_estimator_, '../models/logistic_regression.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5192e32",
   "metadata": {},
   "source": [
    "# 6. K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef096499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "param = {'n_neighbors':[3,5,7,9], 'weights':['uniform','distance']}\n",
    "gs = GridSearchCV(knn, param, cv=5, scoring='f1')\n",
    "gs.fit(X_train_s, y_train)\n",
    "print(gs.best_params_)\n",
    "print(classification_metrics(y_test, gs.predict(X_test_s)))\n",
    "save_model(gs.best_estimator_, '../models/knn.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2e64d",
   "metadata": {},
   "source": [
    "# 7. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "param = [\n",
    "    {'kernel':['linear'], 'C':[0.1,1,10]},\n",
    "    {'kernel':['rbf'], 'C':[0.1,1,10], 'gamma':['scale','auto']}\n",
    "]\n",
    "svc = SVC(probability=True)\n",
    "gs = GridSearchCV(svc, param, cv=5, scoring='roc_auc')\n",
    "gs.fit(X_train_s, y_train)\n",
    "print(gs.best_params_)\n",
    "print(classification_metrics(y_test, gs.predict(X_test_s), gs.predict_proba(X_test_s)[:,1]))\n",
    "save_model(gs.best_estimator_, '../models/svm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b14ff1",
   "metadata": {},
   "source": [
    "# 8. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0353c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "param = {'max_depth':[3,5,10,None], 'min_samples_leaf':[1,2,5]}\n",
    "clf = DecisionTreeClassifier()\n",
    "gs = GridSearchCV(clf, param, cv=5, scoring='f1')\n",
    "gs.fit(X_train_s, y_train)\n",
    "print(gs.best_params_)\n",
    "print(classification_metrics(y_test, gs.predict(X_test_s)))\n",
    "print('Feature importance:', gs.best_estimator_.feature_importances_)\n",
    "save_model(gs.best_estimator_, '../models/decision_tree.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2306dd",
   "metadata": {},
   "source": [
    "# 9. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ca95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "param = {'n_estimators':[100,200], 'max_depth':[None,10,20], 'min_samples_leaf':[1,2]}\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "gs = RandomizedSearchCV(rf, param, n_iter=6, cv=3, scoring='f1')\n",
    "gs.fit(X_train_s, y_train)\n",
    "print(gs.best_params_)\n",
    "print(classification_metrics(y_test, gs.predict(X_test_s)))\n",
    "save_model(gs.best_estimator_, '../models/random_forest.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2be709",
   "metadata": {},
   "source": [
    "# 10. Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6929fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    has_xgb = True\n",
    "except Exception:\n",
    "    has_xgb = False\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "print(classification_metrics(y_test, gb.predict(X_test)))\n",
    "if has_xgb:\n",
    "    xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    print('XGB:', classification_metrics(y_test, xgb_clf.predict(X_test)))\n",
    "    save_model(xgb_clf, '../models/xgboost.joblib')\n",
    "save_model(gb, '../models/gradient_boosting.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5332671",
   "metadata": {},
   "source": [
    "# 11. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=50)),\n",
    "    ('svc', SVC(probability=True))\n",
    "]\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "stack.fit(X_train_s, y_train)\n",
    "print(classification_metrics(y_test, stack.predict(X_test_s), stack.predict_proba(X_test_s)[:,1]))\n",
    "save_model(stack, '../models/stacking.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fb8cb",
   "metadata": {},
   "source": [
    "# 12. Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb318a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "# Try oversampling\n",
    "sm = SMOTE(random_state=42)\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "clf = RandomForestClassifier()\n",
    "p = ImbPipeline([('smote', sm), ('clf', clf)])\n",
    "p.fit(X_train, y_train)\n",
    "print(classification_metrics(y_test, p.predict(X_test)))\n",
    "\n",
    "# Try class-weight handling\n",
    "clf2 = RandomForestClassifier(class_weight='balanced')\n",
    "clf2.fit(X_train, y_train)\n",
    "print(classification_metrics(y_test, clf2.predict(X_test)))\n",
    "\n",
    "# Try undersampling\n",
    "p2 = ImbPipeline([('rus', rus), ('clf', clf)])\n",
    "p2.fit(X_train, y_train)\n",
    "print(classification_metrics(y_test, p2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b8666",
   "metadata": {},
   "source": [
    "# 13. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c736312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='unsupervised', target='target')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_xy(X, y)\n",
    "scaler, X_train_s, X_val_s, X_test_s = scale_data(X_train, X_val, X_test)\n",
    "\n",
    "\n",
    "inertias = []\n",
    "for k in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(2,11), inertias)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('inertia')\n",
    "plt.show()\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "print('Silhouette:', silhouette_score(X, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6deb6",
   "metadata": {},
   "source": [
    "# 14. Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44787eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='unsupervised', target='target')\n",
    "\n",
    "Z = linkage(X, method='ward')\n",
    "plt.figure(figsize=(10,5))\n",
    "dendrogram(Z, truncate_mode='level', p=5)\n",
    "plt.show()\n",
    "\n",
    "labels = fcluster(Z, t=3, criterion='maxclust')\n",
    "print('Cluster counts:', np.bincount(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a815d",
   "metadata": {},
   "source": [
    "# 15. DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2011c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='classification', target='target')\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "dists, _ = nbrs.kneighbors(X)\n",
    "dists = np.sort(dists[:,4])\n",
    "plt.plot(dists)\n",
    "plt.show()\n",
    "\n",
    "db = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = db.fit_predict(X)\n",
    "print('Unique labels:', np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea21a2",
   "metadata": {},
   "source": [
    "# 16. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b37b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data...\n",
    "df = load_csv('../data/dataset.csv')\n",
    "df = basic_clean(df, drop_cols=['id'])\n",
    "X, y = define_xy(df, task_type='unsupervised', target='target')\n",
    "\n",
    "pca = PCA()\n",
    "X_p = pca.fit_transform(X)\n",
    "explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(explained)\n",
    "plt.xlabel('components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n",
    "\n",
    "# Preserve 95% variance\n",
    "n_comp = (explained < 0.95).sum() + 1\n",
    "pca_red = PCA(n_components=n_comp)\n",
    "X_reduced = pca_red.fit_transform(X)\n",
    "print('n_components ->', n_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
